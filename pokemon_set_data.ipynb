{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are uploading all csv files of the Scarlet & Violet main and special sets.\n",
    "\n",
    "Sets: SV01, SV02, SV03, SV: Scarlet & Violet 151, SV04, SV: Paldean Fates, SV05, SV06, SV: Shrouded Fable, SV07, SV08, SV: Prismatic Evolutions, SV09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "sv01_df = pd.read_csv('SV01ScarletAndVioletBaseSetProductsAndPrices.csv')\n",
    "sv02_df = pd.read_csv('SV02PaldeaEvolvedProductsAndPrices.csv')\n",
    "sv03_df = pd.read_csv('SV03ObsidianFlamesProductsAndPrices.csv')\n",
    "sv04_df = pd.read_csv('SV04ParadoxRiftProductsAndPrices.csv')\n",
    "sv05_df = pd.read_csv('SV05TemporalForcesProductsAndPrices.csv')\n",
    "sv06_df = pd.read_csv('SV06TwilightMasqueradeProductsAndPrices.csv')\n",
    "sv07_df = pd.read_csv('SV07StellarCrownProductsAndPrices.csv')\n",
    "sv08_df = pd.read_csv('SV08SurgingSparksProductsAndPrices.csv')\n",
    "sv09_df = pd.read_csv('SV09JourneyTogetherProductsAndPrices.csv')\n",
    "sv_151_df = pd.read_csv('SVScarletAndViolet151ProductsAndPrices.csv')\n",
    "sv_pf_df = pd.read_csv('SVPaldeanFatesProductsAndPrices.csv')\n",
    "sv_sf_df = pd.read_csv('SVShroudedFableProductsAndPrices.csv')\n",
    "sv_pe_df = pd.read_csv('SVPrismaticEvolutionsProductsAndPrices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then merge all of these sets into a master set, filter out unnecessary columns, and add a column for the textual set name based on groupId variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.concat([sv01_df, sv02_df, sv03_df, sv04_df, sv05_df, sv06_df, sv07_df, sv08_df, sv09_df, sv_151_df, sv_pf_df, sv_sf_df, sv_pe_df])\n",
    "master_df = master_df[['productId', 'name', 'groupId', 'lowPrice', 'highPrice', 'marketPrice', 'extRarity']]\n",
    "\n",
    "# filter for cards of specific rarity\n",
    "master_df = master_df[(master_df['extRarity'] == 'Special Illustration Rare') | (master_df['extRarity'] == 'Illustration Rare') | (master_df['extRarity'] == 'Hyper Rare')]\n",
    "\n",
    "# add textual set name column\n",
    "def get_textset(setId):\n",
    "    setId_list = [22873, 23120, 23228, 23286, 23381, 23473, 23537, 23651, 24073, 23237, 23353, 23529, 23821]\n",
    "    textId_list = [\n",
    "        'Scarlet And Violet Base',\n",
    "        'Paldea Evolved',\n",
    "        'Obsidian Flames',\n",
    "        'Paradox Rift',\n",
    "        'Temporal Forces',\n",
    "        'Twilight Masquerade',\n",
    "        'Stellar Crown',\n",
    "        'Surging Sparks',\n",
    "        'Journey Together',\n",
    "        'Scarlet And Violet 151',\n",
    "        'Paldean Fates',\n",
    "        'Shrouded Fable',\n",
    "        'Prismatic Evolutions'\n",
    "    ]\n",
    "    index = setId_list.index(setId)\n",
    "    return textId_list[index]\n",
    "\n",
    "master_df['set'] = master_df['groupId'].apply(get_textset)\n",
    "\n",
    "# add release date\n",
    "def get_dateset(setId):\n",
    "    name_list = ['Scarlet And Violet Base',\n",
    "        'Paldea Evolved',\n",
    "        'Obsidian Flames',\n",
    "        'Paradox Rift',\n",
    "        'Temporal Forces',\n",
    "        'Twilight Masquerade',\n",
    "        'Stellar Crown',\n",
    "        'Surging Sparks',\n",
    "        'Journey Together',\n",
    "        'Scarlet And Violet 151',\n",
    "        'Paldean Fates',\n",
    "        'Shrouded Fable',\n",
    "        'Prismatic Evolutions']\n",
    "    date_list = [\n",
    "        '05-31-2023',\n",
    "        '06-09-2023',\n",
    "        '08-11-2023',\n",
    "        '10-03-2023',\n",
    "        '03-22-2024',\n",
    "        '05-24-2024',\n",
    "        '09-13-2024',\n",
    "        '11-08-2024',\n",
    "        '03-28-2025',\n",
    "        '09-22-2023',\n",
    "        '01-26-2024',\n",
    "        '09-02-2024',\n",
    "        '01-17-2025'\n",
    "    ]\n",
    "    index = name_list.index(setId)\n",
    "    return date_list[index]\n",
    "\n",
    "master_df['set'] = master_df['groupId'].apply(get_textset)\n",
    "master_df['release date'] = master_df['set'].apply(get_dateset)\n",
    "\n",
    "# sort dataframe by card rarity\n",
    "master_df = master_df.sort_values(by=['groupId', 'extRarity'])\n",
    "\n",
    "# reset index\n",
    "master_df = master_df.reset_index()\n",
    "master_df.index +=1\n",
    "\n",
    "master_df['release date'] = pd.to_datetime(master_df['release date'], format='%m-%d-%Y')\n",
    "\n",
    "display(master_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also have to fill in all NaN values, especially for the market prices of recent sets (ie. SV09 Journey Together).\n",
    "\n",
    "In order to fill in the NaN values, we will take the average of the lowPrice and highPrice values and input the average into marketPrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_market_price(row):\n",
    "    if pd.isna(row['marketPrice']):\n",
    "        low_high_average = (row['lowPrice'] + row['highPrice'])/2\n",
    "        row['marketPrice'] = round(low_high_average, 2)\n",
    "    return row['marketPrice']\n",
    "\n",
    "master_df['marketPrice'] = master_df.apply(fill_market_price, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now conduct three hypothesis tests to observe trends and patterns in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST I: Using an ANOVA test, we plan on determining if the average price of cards of the same rarity vary significantly across all relevant sets\n",
    "(relevant sets - scarlet and violet main and special sets)\n",
    "\n",
    "Because we are testing multiple categories and observing if they exist within the same distribution, an ANOVA test would be appropriate.\n",
    "Significance level = 0.05 (Level of confidence = 95%)\n",
    "\n",
    "H0: There is no significant difference in the prices of the same card rarity across the relevant sets.\n",
    "HA: There exists significant differences in the prices of the same card rarity across the relevant sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_df = master_df[master_df['extRarity'] == 'Hyper Rare']\n",
    "ir_df = master_df[master_df['extRarity'] == 'Illustration Rare']\n",
    "sir_df = master_df[master_df['extRarity'] == 'Special Illustration Rare']\n",
    "\n",
    "average_prices = pd.DataFrame(columns=['set', 'averagePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Rare Cards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Hyper Rare cards\n",
    "hr_sets = hr_df.groupby('set', sort=False)\n",
    "\n",
    "for set,group in hr_sets:\n",
    "    group = group.dropna(subset=['marketPrice']).sort_values(by='marketPrice')\n",
    "    rarity_mean = group['marketPrice'].mean()\n",
    "    row = {'set': set, 'averagePrice': round(rarity_mean, 2)}\n",
    "    average_prices = pd.concat([average_prices, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "hr_avg_prices = [group['marketPrice'].dropna().values for _, group in hr_df.groupby('set', sort=False)]\n",
    "\n",
    "hr_res = sp.stats.f_oneway(*hr_avg_prices)\n",
    "hr_tukey = sp.stats.tukey_hsd(*hr_avg_prices)\n",
    "print(hr_res.pvalue)\n",
    "print(hr_tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pairwise T-tests to determine if there exists a significant difference between the volatility in prices between rarities. Here, we define volatility as the difference between a cards high and low price points. We'll have a significance level of $\\alpha = 0.5$. \n",
    "\n",
    "$H_0:$ There is no significant difference in the volatility in prices between rarities. \n",
    "\n",
    "$H_a:$ There is significant difference in the volatility in prices between rarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([             71.01,               40.7,               52.5,\\n       26.009999999999998, 14.049999999999999,              14.93,\\n                     48.0,               80.0,              23.45,\\n                    270.0,\\n       ...\\n                    21.97,  7.350000000000001,              16.01,\\n        7.489999999999998, 19.040000000000003, 45.099999999999994,\\n                    23.55, 6.0600000000000005, 3.9899999999999984,\\n        9.990000000000002],\\n      dtype='float64', length=231)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m ttest_sir_df = master_df[master_df[\u001b[33m'\u001b[39m\u001b[33mextRarity\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mSpecial Illustration Rare\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# df.apply(lambda row: row['Val10']-row['Val1'])\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#ttest_hr_df['volatility'] = ttest_hr_df.apply(lambda x: x['highPrice'] - x ['lowPrice'])\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# [[ttest_hr_df['highPrice']]-ttest_hr_df['lowPrice']]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m ttest_ir_df[\u001b[33m'\u001b[39m\u001b[33mvolatility\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mttest_ir_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mttest_ir_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhighPrice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m-\u001b[49m\u001b[43mttest_ir_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlowPrice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m ttest_sir_df[\u001b[33m'\u001b[39m\u001b[33mvolatility\u001b[39m\u001b[33m'\u001b[39m] = ttest_sir_df[ttest_sir_df[\u001b[33m'\u001b[39m\u001b[33mhighPrice\u001b[39m\u001b[33m'\u001b[39m]-ttest_sir_df[\u001b[33m'\u001b[39m\u001b[33mlowPrice\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index([             71.01,               40.7,               52.5,\\n       26.009999999999998, 14.049999999999999,              14.93,\\n                     48.0,               80.0,              23.45,\\n                    270.0,\\n       ...\\n                    21.97,  7.350000000000001,              16.01,\\n        7.489999999999998, 19.040000000000003, 45.099999999999994,\\n                    23.55, 6.0600000000000005, 3.9899999999999984,\\n        9.990000000000002],\\n      dtype='float64', length=231)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_hr_df = master_df[master_df['extRarity'] == 'Hyper Rare']\n",
    "ttest_ir_df = master_df[master_df['extRarity'] == 'Illustration Rare']\n",
    "ttest_sir_df = master_df[master_df['extRarity'] == 'Special Illustration Rare']\n",
    "\n",
    "# df.apply(lambda row: row['Val10']-row['Val1'])\n",
    "#ttest_hr_df['volatility'] = ttest_hr_df.apply(lambda x: x['highPrice'] - x ['lowPrice'])\n",
    "# [[ttest_hr_df['highPrice']]-ttest_hr_df['lowPrice']]\n",
    "ttest_ir_df['volatility'] = ttest_ir_df[ttest_ir_df['highPrice']-ttest_ir_df['lowPrice']]\n",
    "ttest_sir_df['volatility'] = ttest_sir_df[ttest_sir_df['highPrice']-ttest_sir_df['lowPrice']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
